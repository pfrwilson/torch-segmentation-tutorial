{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f4d2d605",
      "metadata": {
        "id": "f4d2d605"
      },
      "source": [
        "# Live Demo: Training a 3DÂ UNet with MONAI"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up environment"
      ],
      "metadata": {
        "id": "zJ0GYiznpQjd"
      },
      "id": "zJ0GYiznpQjd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "823c066e",
      "metadata": {
        "id": "823c066e"
      },
      "outputs": [],
      "source": [
        "!pip -q install \"monai-weekly\" \"torch>=2.1\" \"tqdm\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "538b2df8",
      "metadata": {
        "id": "538b2df8"
      },
      "outputs": [],
      "source": [
        "import monai, torch, os, tempfile, matplotlib.pyplot as plt\n",
        "from monai.data import DataLoader, CacheDataset\n",
        "from monai.networks.nets import UNet\n",
        "from monai.losses import DiceLoss\n",
        "from monai.metrics import DiceMetric"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the dataset"
      ],
      "metadata": {
        "id": "RzquBe3ppUt0"
      },
      "id": "RzquBe3ppUt0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6de70f40",
      "metadata": {
        "id": "6de70f40"
      },
      "outputs": [],
      "source": [
        "\n",
        "from monai.apps import download_and_extract\n",
        "root_dir = tempfile.mkdtemp()\n",
        "resource = 'https://msd-for-monai.s3-us-west-2.amazonaws.com/Task09_Spleen.tar'\n",
        "compressed_file = os.path.join(root_dir, \"spleen.tar\")\n",
        "download_and_extract(resource, compressed_file, root_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the dataset"
      ],
      "metadata": {
        "id": "juwbkqtjpXDd"
      },
      "id": "juwbkqtjpXDd"
    },
    {
      "cell_type": "code",
      "source": [
        "from monai.data import Dataset\n",
        "from glob import glob\n",
        "images = sorted(glob(os.path.join(root_dir,\"Task09_Spleen/imagesTr/*.nii.gz\")))\n",
        "labels = sorted(glob(os.path.join(root_dir,\"Task09_Spleen/labelsTr/*.nii.gz\")))\n",
        "train_files = [{\"image\":img, \"label\":lbl} for img,lbl in zip(images,labels)]\n",
        "\n",
        "print(f\"Number of files: {len(train_files)}\")\n",
        "print(train_files[0])\n"
      ],
      "metadata": {
        "id": "Mvl6ztcDpJ5w"
      },
      "id": "Mvl6ztcDpJ5w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from monai import transforms as T\n",
        "\n",
        "def make_transform(n_pixels=64):\n",
        "    # Steps to transform the file information to model inputs:\n",
        "\n",
        "    transforms = []\n",
        "\n",
        "    # step 1: load the data, nii.gz format to tensor\n",
        "    transforms.append(\n",
        "        T.LoadImaged(keys=['image', 'label'])\n",
        "    )\n",
        "\n",
        "    # step 2: Add an extra \"channel\" dimension (pytorch/monai convention)\n",
        "    transforms.append(\n",
        "        T.EnsureChannelFirstd(keys=['image', 'label'])\n",
        "    )\n",
        "\n",
        "    # step 3: Resize the data to a uniform size\n",
        "    transforms.append(\n",
        "        T.ResizeD(keys=['image', 'label'], spatial_size=(n_pixels, n_pixels, n_pixels//2), mode=['bilinear', 'nearest'])\n",
        "    )\n",
        "\n",
        "    # step 4: rescale the image intenisty between 0 and 1\n",
        "    transforms.append(T.ScaleIntensityD(keys=['image']))\n",
        "\n",
        "    transform = T.Compose(transforms)\n",
        "    return transform\n",
        "\n",
        "\n",
        "transform = make_transform(256)"
      ],
      "metadata": {
        "id": "Xx2pAvQkqDqM"
      },
      "id": "Xx2pAvQkqDqM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look at the output\n",
        "data = transform(train_files[0])\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(data['image'][0, ..., 75])\n",
        "ax[1].imshow(data['image'][0, ..., 75])\n",
        "ax[1].imshow(data['label'][0, ..., 75], alpha=0.5)\n",
        "fig.tight_layout()\n",
        "\n",
        "image = data['image']\n",
        "label = data['label']\n",
        "print(f\"pixel mean: {image.mean()}\")\n",
        "print(f\"pixel std: {image.std()}\")\n",
        "print(f\"Image shape: {image.shape}\")\n",
        "print(f\"Label shape: {label.shape}\")\n",
        "print(f\"Label values: {torch.unique(label)}\")"
      ],
      "metadata": {
        "id": "aYJY-yrjsqVY"
      },
      "id": "aYJY-yrjsqVY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataset and dataloader\n",
        "train_ds = train_dataset = monai.data.CacheDataset(train_files, transform=make_transform(64), cache_rate=1)\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=4, shuffle=True)\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "image = batch['image']\n",
        "label = batch['label']\n",
        "print(f'Image shape: {image.shape}')\n",
        "print(f'Label shape: {label.shape}')\n"
      ],
      "metadata": {
        "id": "dyEuuXUOusxk"
      },
      "id": "dyEuuXUOusxk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the model for training"
      ],
      "metadata": {
        "id": "u8oBRBgavUZQ"
      },
      "id": "u8oBRBgavUZQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# define a unet using monai\n",
        "\n",
        "model = UNet(spatial_dims=3, in_channels=1, out_channels=2,\n",
        "             channels=(16,32,64,128), strides=(2,2,2),\n",
        "             num_res_units=2)\n",
        "\n",
        "n_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model has {n_params} parameters.\")"
      ],
      "metadata": {
        "id": "KzT1WPO-vfqh"
      },
      "id": "KzT1WPO-vfqh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dee78eb",
      "metadata": {
        "id": "3dee78eb"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # device to run calculations - \"cuda\" means gpu\n",
        "print(f\"Using device {device}\")\n",
        "model.to(device) # convert model to the correct device\n",
        "loss_fn = DiceLoss(to_onehot_y=True, softmax=True)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "218204a3",
      "metadata": {
        "id": "218204a3"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "max_epochs = 100\n",
        "for epoch in range(max_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch_data in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
        "        inputs, labels = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, avg loss: {epoch_loss/len(train_loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e1af18c",
      "metadata": {
        "id": "7e1af18c"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    sample = train_ds[0]\n",
        "    input_volume = sample[\"image\"].unsqueeze(0).to(device)\n",
        "    pred = torch.argmax(model(input_volume), dim=1).cpu()[0]\n",
        "\n",
        "import numpy as np, matplotlib.pyplot as plt\n",
        "mid_slice = pred.shape[-1]//2\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,3,1); plt.imshow(input_volume.cpu()[0,0,:, :, mid_slice], cmap='gray'); plt.title('Image'); plt.axis('off')\n",
        "plt.subplot(1,3,2); plt.imshow(sample[\"label\"][0, :, :, mid_slice], cmap='gray'); plt.title('Ground truth'); plt.axis('off')\n",
        "plt.subplot(1,3,3); plt.imshow(pred[:, :, mid_slice], cmap='gray'); plt.title('Prediction'); plt.axis('off')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}